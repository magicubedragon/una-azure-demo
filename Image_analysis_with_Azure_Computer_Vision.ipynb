{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuj8_Fb5agt7",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Image analysis with Azure Computer Vision "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYJMFEUTagt-",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1. Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3btvWdWbg7G",
        "outputId": "eb192e18-5eef-4af2-bad0-9fc698c91da9"
      },
      "outputs": [],
      "source": [
        "!pip3 install azure-cognitiveservices-vision-computervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyUwqYn1cBZ8"
      },
      "outputs": [],
      "source": [
        "def show_image_caption(image_path, description):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from PIL import Image\n",
        "\n",
        "    # Display the image\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    img = Image.open(image_path)\n",
        "    caption_text = ''\n",
        "    if (len(description.captions) == 0):\n",
        "        caption_text = 'No caption detected'\n",
        "    else:\n",
        "        for caption in description.captions:\n",
        "            caption_text = caption_text + \" '{}'\\n(Confidence: {:.2f}%)\".format(caption.text, caption.confidence * 100)\n",
        "    plt.title(caption_text)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "\n",
        "def show_image_analysis(image_path, analysis):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from PIL import Image, ImageDraw\n",
        "    import numpy as np\n",
        "\n",
        "    # Display the image\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "    a = fig.add_subplot(1,2,1)\n",
        "    img = Image.open(image_path)\n",
        "    \n",
        "    # Get the caption\n",
        "    caption_text = ''\n",
        "    if (len(analysis.description.captions) == 0):\n",
        "        caption_text = 'No caption detected'\n",
        "    else:\n",
        "        for caption in analysis.description.captions:\n",
        "            caption_text = caption_text + \" '{}'\\n(Confidence: {:.2f}%)\".format(caption.text, caption.confidence * 100)\n",
        "    plt.title(caption_text)\n",
        "\n",
        "    # Get objects\n",
        "    if analysis.objects:\n",
        "        # Draw a rectangle around each object\n",
        "        for object in analysis.objects:\n",
        "            r = object.rectangle\n",
        "            bounding_box = ((r.x, r.y), (r.x + r.w, r.y + r.h))\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            draw.rectangle(bounding_box, outline='magenta', width=5)\n",
        "            plt.annotate(object.object_property,(r.x, r.y), backgroundcolor='magenta')\n",
        "\n",
        "    # Get faces\n",
        "    if analysis.faces:\n",
        "        # Draw a rectangle around each face\n",
        "        for face in analysis.faces:\n",
        "            r = face.face_rectangle\n",
        "            bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            draw.rectangle(bounding_box, outline='lightgreen', width=5)\n",
        "            annotation = 'Person aged approxilately {}'.format(face.age)\n",
        "            plt.annotate(annotation,(r.left, r.top), backgroundcolor='lightgreen')\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "\n",
        "    # Add a second plot for addition details\n",
        "    a = fig.add_subplot(1,2,2)\n",
        "\n",
        "    # Get ratings\n",
        "    ratings = 'Ratings:\\n - Adult: {}\\n - Racy: {}\\n - Gore: {}'.format(analysis.adult.is_adult_content,\n",
        "                                                                           analysis.adult.is_racy_content,\n",
        "                                                                           analysis.adult.is_gory_content,)\n",
        "\n",
        "    # Get tags\n",
        "    tags = 'Tags:'\n",
        "    for tag in analysis.tags:\n",
        "        tags = tags + '\\n - {}'.format(tag.name)\n",
        "\n",
        "    # Print details\n",
        "\n",
        "    details = '{}\\n\\n{}'.format(ratings, tags)\n",
        "    a.text(0,0.4, details, fontsize=12)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrGWTYhlaguA",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2. Setup Key and Endpoint\n",
        "\n",
        "To use your cognitive services resource, client applications need its endpoint and authentication key:\n",
        "\n",
        "1. In the Azure portal, on the **Keys and Endpoint** page for your cognitive service resource, copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n",
        "2. Copy the **endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n",
        "3. Run the code below by selecting the cell and then clicking the **Run cell** (&#9655) button to the left of the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "gather": {
          "logged": 1626334044645
        },
        "id": "Q6s0Lr2daguE",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "00b38930-42dd-4dad-c582-9604484358ba"
      },
      "outputs": [],
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVVowyIGaguF",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now that you've set up the key and endpoint, you can use the computer vision service to analyze an image.\n",
        "\n",
        "Run the following cell to get a description for an image in the */data/vision/store_cam1.jpg* file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "collapsed": true,
        "gather": {
          "logged": 1626333707293
        },
        "id": "LGqPRzkAaguF",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "4144db0e-88b6-45c0-86ea-d9c492e77d89"
      },
      "outputs": [],
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get the path to an image file\n",
        "image_path = os.path.join('store_cam1.jpg')\n",
        "\n",
        "# Get a client for the computer vision service\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Get a description from the computer vision service\n",
        "image_stream = open(image_path, \"rb\")\n",
        "description = computervision_client.describe_image_in_stream(image_stream)\n",
        "\n",
        "# Display image and caption (code in helper_scripts/vision.py)\n",
        "show_image_caption(image_path, description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PybPzvL5aguF",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "That seems reasonably accurate.\n",
        "\n",
        "Let's try another image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "collapsed": true,
        "gather": {
          "logged": 1626330976554
        },
        "id": "ZaD_s26daguF",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "ab0d4e69-f537-4f8f-aa9a-fdf587f6d1c9"
      },
      "outputs": [],
      "source": [
        "# Get the path to an image file\n",
        "image_path = os.path.join('store_cam1.jpg')\n",
        "\n",
        "# Get a description from the computer vision service\n",
        "image_stream = open(image_path, \"rb\")\n",
        "description = computervision_client.describe_image_in_stream(image_stream)\n",
        "\n",
        "# Display image and caption (code in helper_scripts/vision.py)\n",
        "show_image_caption(image_path, description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm0QZ6A-aguF",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Again, the suggested caption seems to be pretty accurate.\n",
        "\n",
        "## Analyze image features\n",
        "\n",
        "So far, you've used the Computer Vision service to generate a descriptive caption for a couple of images; but there's much more you can do. The Computer Vision service provides analysis capabilities that can extract detailed information like:\n",
        "\n",
        "- The locations of common types of object detected in the image.\n",
        "- Location and approximate age of human faces in the image.\n",
        "- Whether the image contains any 'adult', 'racy', or 'gory' content.\n",
        "- Relevant tags that could be associated with the image in a database to make it easy to find.\n",
        "\n",
        "Run the following code to analyze an image of a shopper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "collapsed": true,
        "gather": {
          "logged": 1626330980761
        },
        "id": "wGKJGfKLaguG",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "69b80904-627f-4bca-f27d-07a6114c54e8"
      },
      "outputs": [],
      "source": [
        "# Get the path to an image file\n",
        "image_path = os.path.join('store_cam1.jpg')\n",
        "\n",
        "# Specify the features we want to analyze\n",
        "features = ['Description', 'Tags', 'Adult', 'Objects', 'Faces']\n",
        "\n",
        "# Get an analysis from the computer vision service\n",
        "image_stream = open(image_path, \"rb\")\n",
        "analysis = computervision_client.analyze_image_in_stream(image_stream, visual_features=features)\n",
        "\n",
        "# Show the results of analysis (code in helper_scripts/vision.py)\n",
        "show_image_analysis(image_path, analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyifqQiSaguG",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Learn More\n",
        "\n",
        "In addition to the capabilities you've explored in this notebook, the Computer Vision cognitive service includes the ability to:\n",
        "\n",
        "- Identify celebrities in images.\n",
        "- Detect brand logos in an image.\n",
        "- Perform optical character recognition (OCR) to read text in an image.\n",
        "\n",
        "To learn more about the Computer Vision cognitive service, see the [Computer Vision documentation](https://docs.microsoft.com/azure/cognitive-services/computer-vision/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\n",
        "cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\n",
        "cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\n",
        "\n",
        "model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yE3ZrIkraguG",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
        "from msrest.authentication import ApiKeyCredentials\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Load a test image and get its dimensions\n",
        "test_img_file = os.path.join('produce.jpg')\n",
        "test_img = Image.open(test_img_file)\n",
        "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\n",
        "\n",
        "# Get a prediction client for the object detection model\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\n",
        "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\n",
        "\n",
        "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\n",
        "\n",
        "# Detect objects in the test image\n",
        "with open(test_img_file, mode=\"rb\") as test_data:\n",
        "    results = predictor.detect_image(project_id, model_name, test_data)\n",
        "\n",
        "# Create a figure to display the results\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.axis('off')\n",
        "\n",
        "# Display the image with boxes around each detected object\n",
        "draw = ImageDraw.Draw(test_img)\n",
        "lineWidth = int(np.array(test_img).shape[1]/100)\n",
        "object_colors = {\n",
        "    \"apple\": \"lightgreen\",\n",
        "    \"banana\": \"yellow\",\n",
        "    \"orange\": \"orange\"\n",
        "}\n",
        "for prediction in results.predictions:\n",
        "    color = 'white' # default for 'other' object tags\n",
        "    if (prediction.probability*100) > 50:\n",
        "        if prediction.tag_name in object_colors:\n",
        "            color = object_colors[prediction.tag_name]\n",
        "        left = prediction.bounding_box.left * test_img_w \n",
        "        top = prediction.bounding_box.top * test_img_h \n",
        "        height = prediction.bounding_box.height * test_img_h\n",
        "        width =  prediction.bounding_box.width * test_img_w\n",
        "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\n",
        "        draw.line(points, fill=color, width=lineWidth)\n",
        "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\n",
        "plt.imshow(test_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_faces(image_path, detected_faces, show_id=False):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from PIL import Image, ImageDraw\n",
        "\n",
        "    # Open an image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Create a figure to display the results\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "\n",
        "    if detected_faces:\n",
        "        # If there are faces, how many?\n",
        "        num_faces = len(detected_faces)\n",
        "        prediction = ' (' + str(num_faces) + ' faces detected)'\n",
        "        # Draw a rectangle around each detected face\n",
        "        for face in detected_faces:\n",
        "            r = face.face_rectangle\n",
        "            bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            draw.rectangle(bounding_box, outline='magenta', width=5)\n",
        "            if show_id:\n",
        "                plt.annotate(face.face_id,(r.left, r.top + r.height + 15), backgroundcolor='white')\n",
        "        #a = fig.add_subplot(1,1,1)\n",
        "        fig.suptitle(prediction)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "\n",
        "def show_face_attributes(image_path, detected_faces):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from PIL import Image, ImageDraw\n",
        "\n",
        "    # Open an image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Create a figure to display the results\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "\n",
        "    if detected_faces:\n",
        "        # If there are faces, how many?\n",
        "        num_faces = len(detected_faces)\n",
        "        prediction = ' (' + str(num_faces) + ' faces detected)'\n",
        "        # Draw a rectangle around each detected face\n",
        "        for face in detected_faces:\n",
        "            r = face.face_rectangle\n",
        "            bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            draw.rectangle(bounding_box, outline='magenta', width=5)\n",
        "\n",
        "            # Annotate with face attributes (only age and emotion are used in this sample)\n",
        "            detected_attributes = face.face_attributes.as_dict()\n",
        "            age = 'age unknown' if 'age' not in detected_attributes.keys() else int(detected_attributes['age'])\n",
        "            annotations = 'Person aged approximately {}'.format(age)\n",
        "            txt_lines = 1\n",
        "            if 'emotion' in detected_attributes.keys():\n",
        "                for emotion_name in detected_attributes['emotion']:\n",
        "                    txt_lines += 1\n",
        "                    annotations += '\\n - {}: {}'.format(emotion_name, detected_attributes['emotion'][emotion_name])\n",
        "            plt.annotate(annotations,((r.left + r.width), (r.top + r.height + (txt_lines * 12))), backgroundcolor='white')\n",
        "\n",
        "        # Plot the image\n",
        "        #a = fig.add_subplot(1,1,1)\n",
        "        fig.suptitle(prediction)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "\n",
        "def show_similar_faces(image_1_path, image_1_face, image_2_path, image_2_faces, similar_faces):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from PIL import Image, ImageDraw\n",
        "\n",
        "    # Create a figure to display the results\n",
        "    fig = plt.figure(figsize=(16, 6))\n",
        "\n",
        "    # Show face 1\n",
        "    img1 = Image.open(image_1_path)\n",
        "    r = image_1_face.face_rectangle\n",
        "    bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
        "    draw = ImageDraw.Draw(img1)\n",
        "    draw.rectangle(bounding_box, outline='magenta', width=5)\n",
        "    a = fig.add_subplot(1,2,1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img1)\n",
        "\n",
        "    # get the matching face IDs\n",
        "    matching_face_ids = list(map(lambda face: face.face_id, similar_faces))\n",
        "\n",
        "    # Draw a rectangle around each similar face in image 2\n",
        "    img2 = Image.open(image_2_path)\n",
        "    a = fig.add_subplot(1,2,2)\n",
        "    plt.axis('off')\n",
        "    for face in image_2_faces:\n",
        "        r = face.face_rectangle\n",
        "        bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
        "        draw = ImageDraw.Draw(img2)\n",
        "        if face.face_id in matching_face_ids:\n",
        "            draw.rectangle(bounding_box, outline='lightgreen', width=10)\n",
        "            plt.annotate('Match!',(r.left, r.top + r.height + 15), backgroundcolor='white')\n",
        "        else:\n",
        "            draw.rectangle(bounding_box, outline='red', width=5)\n",
        "    plt.imshow(img2)\n",
        "    plt.show()\n",
        "\n",
        "def show_recognized_faces(image_path, detected_faces, recognized_face_names):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from PIL import Image, ImageDraw\n",
        "\n",
        "    # Open an image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Create a figure to display the results\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "\n",
        "    if detected_faces:\n",
        "        # If there are faces, how many?\n",
        "        num_faces = len(recognized_face_names)\n",
        "        caption = ' (' + str(num_faces) + ' faces recognized)'\n",
        "        # Draw a rectangle around each detected face\n",
        "        for face in detected_faces:\n",
        "            r = face.face_rectangle\n",
        "            bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            draw.rectangle(bounding_box, outline='magenta', width=5)\n",
        "            if face.face_id in recognized_face_names:\n",
        "                plt.annotate(recognized_face_names[face.face_id],\n",
        "                             (r.left, r.top + r.height + 15), backgroundcolor='white')\n",
        "        #a = fig.add_subplot(1,1,1)\n",
        "        fig.suptitle(caption)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from python_code import faces\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Create a face detection client.\n",
        "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Open an image\n",
        "image_path = os.path.join('store_cam2.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detect faces\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# Display the faces (code in python_code/faces.py)\n",
        "faces.show_faces(image_path, detected_faces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open an image\n",
        "image_path = os.path.join('store_cam3.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detect faces\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# Display the faces (code in python_code/faces.py)\n",
        "faces.show_faces(image_path, detected_faces, show_id=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open an image\n",
        "image_path = os.path.join('store_cam1.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detect faces and specified facial attributes\n",
        "attributes = ['age', 'emotion']\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
        "\n",
        "# Display the faces and attributes (code in python_code/faces.py)\n",
        "faces.show_face_attributes(image_path, detected_faces)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "01 - Image analysis with Azure Computer Vision.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
